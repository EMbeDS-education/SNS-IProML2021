{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><font size=\"+4\">Introduction to Programming and Machine Learning in Python 2020/2021</font></center>\n",
    "<center><font size=\"+4\">Module 2</font></center>\n",
    "<center><font size=\"+2\">Scuola Normale Superiore, Pisa, Italy</font></center>\n",
    "\n",
    "<center><font size=\"+2\">Course responsibles</font></center>\n",
    "<center><font size=\"+2\">Andrea Vandin a.vandin@santannapisa.it</font></center>\n",
    "<center><font size=\"+2\">Daniele Licari d.licari@santannapisa.it</font></center>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><font size=\"+2\">Part 2</font></center>\n",
    "<center><font size=\"+1\">Breast Cancer Diagnosis 1</font></center>\n",
    "<center><font size=\"+1\">Overview of data processing & unsupervised learning</font></center>\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This notebook provides an overview of unsupervised learning pipeline**\n",
    "   * Exploratory Data Analysis (data pre-processing, missing values, outliers detection, ...)\n",
    "   * Dimensionality Reduction (PCA)\n",
    "   * Clustering (K-means)\n",
    "\n",
    "You can find more details in the [APPENDIX](#APPENDIX) of this document.\n",
    "\n",
    "In particular, this notebook will introduce the libraries:\n",
    "\n",
    "   * [scikit-learn](https://scikit-learn.org/stable/): simple and efficient tools for predictive data analysis \n",
    "   * [Seaborn](http://seaborn.pydata.org/): seaborn is a Python data visualization library based on matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**References** \n",
    "\n",
    "Some in-depth study material:\n",
    "\n",
    "* <mark> [Statistics and Machine Learning in Python, E.Duchesnay, T.Löfstedt, F.Younes](https://duchesnay.github.io/pystatsml)</mark>\n",
    "* [Topics in Statistical Learning, Francesca Chiaromonte](https://github.com/EMbeDS-education/StatsAndComputing20202021/tree/main/TSL/slides)\n",
    "* [Python for Data Analysis, 2nd edition, William Wesley McKinney (O’Reilly)](https://www.oreilly.com/library/view/python-data-science/9781491912126/)\n",
    "* [Freely available Jupyter notebooks covering the examples/material of each chapter](https://github.com/jakevdp/PythonDataScienceHandbook/tree/master/notebooks)\n",
    "* [Introduction to Data Mining (2nd Edition), Pang-Ning Tan et al.](https://www.cse.msu.edu/~ptan/)\n",
    "* [Introduction to Machine Learning Algorithms, KNIME AG](https://www.knime.com/knime-course-material-download-page)\n",
    "\n",
    "Some pictures have been taken from these sources.\n",
    "\n",
    "# What Is Machine Learning?\n",
    "\n",
    "<img src=\"images/whats_ml.jpg\" alt=\"ML\" style=\"width: 400px;\"/>\n",
    "\n",
    "Machine learning can be categorized into two main types: supervised learning and unsupervised learning.\n",
    "\n",
    "- Supervised learning models tries to learn the relationship between measured features X of data and some labels y associated with the data; The learning algorithm adjusts (learns) the model parameters through a number of iterations to maximize/minimize a likelihood/error function on output. Once this model is determined, it can be used to apply labels to new, unknown data. This is further subdivided into:\n",
    "  - *Classification* tasks, the labels are discrete categories\n",
    "  - *Regression* tasks, the labels are continuous quantities. \n",
    "\n",
    "- Unsupervised learning involves modeling the features of a dataset X without reference to any label, and is often described as “letting the dataset speak for itself.” These models include tasks such as:\n",
    "    - *Outlier Detection* is the identification of rare items\n",
    "    - *Dimensionality Reduction* search for a lower-dimensional representations of the data.\n",
    "    - *Clustering* identify distinct groups of data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# Benign and Malignant Breast Cancer Case Study \n",
    "We will analize Wisconsin Breast Cancer Dataset (WBCD), features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image.\n",
    "\n",
    "<img src=\"images/Breast-Biopsy-2.jpg\" >\n",
    "\n",
    "![alt text](images/fna-benign1.png)\n",
    "![alt text](images/fna-malignant1.png)\n",
    "\n",
    "**Attribute Information**\n",
    "- radius (mean of distances from center to points on the perimeter)\n",
    "- texture (standard deviation of gray-scale values)\n",
    "- perimeter\n",
    "- area\n",
    "- smoothness (local variation in radius lengths)\n",
    "- compactness (perimeter^2 / area - 1.0)\n",
    "- concavity (severity of concave portions of the contour)\n",
    "- concave points (number of concave portions of the contour)\n",
    "- symmetry \n",
    "- fractal dimension (\"coastline approximation\" - 1)\n",
    "\n",
    "The mean, standard error, and \"worst\" or largest (mean of the three\n",
    "largest values) of these features were computed for each image,\n",
    "resulting in 30 features.  For instance, field 3 is Mean Radius, field\n",
    "13 is Radius SE, field 23 is Worst Radius.\n",
    "\n",
    "**Labels Class:**\n",
    "* malignant\n",
    "* benign\n",
    "\n",
    "\n",
    "\n",
    "This dataset is also available via the ftp server UW CS: http://ftp.cs.wisc.edu/math-prog/cpo-dataset/machine-learn/cancer/WDBC/\n",
    "\n",
    "![](images/machine_learning_cancer.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "We will start by installing the necessary tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    " # numpy for numerical computing\n",
    "!{sys.executable} -m pip install numpy\n",
    "# pandas for data processing\n",
    "!{sys.executable} -m pip install pandas \n",
    " # seaborn for visualization\n",
    "!{sys.executable} -m pip install seaborn\n",
    "#sklearn for machine learning \n",
    "!{sys.executable} -m pip install sklearn \n",
    "# scipy for statistical functions \n",
    "!{sys.executable} -m pip install scipy  \n",
    "# statsmodels for statistical models\n",
    "!{sys.executable} -m pip install statsmodels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Processing libs\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Visualizzation libs\n",
    "# keeps the plots in one place. calls image as static pngs\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt # side-stepping mpl backend\n",
    "import seaborn as sns # data visualization library based on matplotlib\n",
    "from IPython.display import display, Markdown # display Markdown code using Python\n",
    "\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn comes with a [few standard datasets](https://scikit-learn.org/stable/datasets/toy_dataset.html), we can load the Wisconsin Breast Cancer Dataset (WBCD) using *sklearn.datasets.load_breast_cancer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer # to load the dataset\n",
    "\n",
    "# load dataset \n",
    "data = load_breast_cancer() \n",
    "# data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Features matrix\n",
    "\n",
    "**Here each row of the data refers to a single observed biopsy sample**, and the number of rows is the total number of samples in the dataset. Likewise, each column of the data refers to a particular quantitative piece of information\n",
    "that describes each sample. In general, we will refer to the columns of the matrix as features, and the number of columns as n_features.\n",
    "By convention, **this features matrix is often stored in a variable named X**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features matrix \n",
    "df_X = pd.DataFrame(data.data,columns=data.feature_names) \n",
    "# feature DataFrame\n",
    "print(df_X.shape)\n",
    "\n",
    "df_X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target array\n",
    "In addition to the feature matrix X, we also generally work with a **label or target array**,\n",
    "which by convention we will usually call ***y***. The target array is usually one dimensional,\n",
    "with length n_samples. The target array **may have continuous numerical values, or discrete\n",
    "classes/labels**\n",
    "\n",
    "For example, in the our dataset we may wish to construct a model that can predict the\n",
    "type of breast cancer based on the features measurements\n",
    "\n",
    "**Diagnosis Class:**\n",
    "* malignant\n",
    "* benign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for t in data.target:\n",
    "#     print(data.target_names[t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_y = pd.Series([data.target_names[i] for i in data.target], name=\"target\")\n",
    "print(df_y.shape)\n",
    "df_y.sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data layout: df_X and df_y\n",
    "![](images/05.02-samples-features.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA)\n",
    "Exploratory data analysis (EDA) is used by data scientists to analyze and investigate data sets and summarize their main characteristics, \n",
    "**It use data visualization methods to discover patterns, spot anomalies**, test a hypothesis, or check assumptions.\n",
    "It provides a better understanding of data set variables and the relationships between them\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Encoding Categorical Values\n",
    "![](images/categorical_var.png)\n",
    "**Some data sets can contain categorical variables**. These variables are typically stored as text values which represent various traits. For example the temperature (\"high\", \"medium\", \"low\") or city name (\"paris\", \"tokyo\", \"milan\").\n",
    "**Many machine learning algorithms can support categorical values** without further manipulation **but there are many more algorithms that do not**. Therefore, we should convert these text attributes into numerical values for further processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Find and replace*: Pandas makes it easy for us to directly replace the text values with their numeric equivalent by using replace ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode target labels with value 1 for malignant and 0 for benign\n",
    "dict_lb_to_num = {'malignant':1, 'benign':0} # set malignat as true class\n",
    "df_y.replace(dict_lb_to_num).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[*sklearn.preprocessing.LabelEncoder*](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) is a utility class to transform non-numerical labels to numerical labels such that they contain only values between 0 and n_classes-1. \n",
    "This transformer should be used to encode target values, i.e. y, and not the input X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import  LabelEncoder\n",
    "# convert categorical variable (label) into numerical variable\n",
    "le = LabelEncoder()\n",
    "le.fit_transform(df_y)[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert categorical nominal variable (feature) into numerical variable\n",
    "df_performance = pd.Series([\"good\", \"bad\", \"optimal\"], name='performance')\n",
    "le.fit_transform(df_performance)[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nominal categorical variables\n",
    "df_city= pd.DataFrame({'city':[\"paris\", \"paris\", \"tokyo\", \"milan\"],'number':[2,3,4,5]})\n",
    "df_city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# le.fit_transform(df_city.city)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[*pandas.get_dummies*](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html) convert categorical variable into dummy/indicator variables. It takes only the value 0 or 1 to indicate the absence or presence of categorical values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it also called one-hot encoding which creates a binary variable (0/1) for each possible value,\n",
    "features_encoded = pd.get_dummies(df_city, columns=['city'])\n",
    "features_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.spatial import distance\n",
    "# paris = (0,1,0)\n",
    "# milan = (1,0,0)\n",
    "# tokyo = (0,0,1)\n",
    "# print(distance.euclidean(paris, milan))\n",
    "# print(distance.euclidean(milan, tokyo))\n",
    "# print(distance.euclidean(milan, tokyo))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<marquee style='width: 30%; color: red;'><b>Important! Golden Rules</b></marquee>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" style='color:black'>\n",
    "<b>Tip</b>      \n",
    "We apply get_dummies when:\n",
    "\n",
    "* The categorical (Nominal) variables are in features matrix\n",
    "\n",
    "We apply Label Encoding or Replace when:\n",
    "* The categorical variable is target array\n",
    "* The categorical (Ordinal) variables are in features matrix\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Bin values into discrete intervals\n",
    "\n",
    "When dealing with continuous numeric data, it is often helpful to bin the data into multiple buckets for further analysis.\n",
    "\n",
    "Pandas supports these approaches using the cut and qcut function:\n",
    " * [cut](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.cut.html): Use cut when you need to segment and sort data values into bins. This function is also useful for going from a continuous variable to a categorical variable. For example, cut could convert ages to groups of age ranges.\n",
    " * [qcut](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.qcut.html): Discretize variable into equal-sized buckets, based on sample quantiles\n",
    " \n",
    " ![](images/binning.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_age = pd.DataFrame({'Age': np.random.normal(30, 2, 1000) })\n",
    "fig, ax = plt.subplots()\n",
    "df_age['Age'].hist(bins=10, color='#A9C5D3', edgecolor='black', grid=False)\n",
    "ax.set_title('Class Age Histogram', fontsize=12)\n",
    "ax.set_xlabel('Age', fontsize=12)\n",
    "ax.set_ylabel('Frequency', fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_age['bucket_w'] = pd.cut(df_age.Age, 3) \n",
    "df_age['bucket_w'].value_counts()\n",
    "print(df_age['bucket_w'].value_counts())# .plot(kind='bar')\n",
    "\n",
    "df_age['bucket_q'] = pd.qcut(df_age.Age, 3) \n",
    "print(df_age['bucket_q'].value_counts())# .plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_age['bucket_q'] = pd.cut(df_age.Age, 3,  labels=[\"Student\", \"PhD Student\", \"Post-Doc\"]) \n",
    "print(df_age['bucket_q'].value_counts())# .plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " pd.cut(df_age.Age, [10,20,30,40]).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values\n",
    "**It is not unusual for an object to be missing one or more attribute values.** In some cases, the information was not collected; while in other cases, some attributes are inapplicable to the data instances. This section presents examples on the different approaches for handling missing values.\n",
    "\n",
    "**Pandas treats None and NaN as essentially interchangeable for indicating missing or null values.** To facilitate this convention, there are several useful methods for detecting, removing, and replacing null values in Pandas data structures. They are:\n",
    "\n",
    "* [isnull()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.isnull.html): Generate a boolean mask indicating missing values\n",
    "* [notnull()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.notnull.html): Opposite of isnull()\n",
    "* [dropna()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html): Return a filtered version of the data\n",
    "* [fillna()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.fillna.html): Return a copy of the data with missing values filled "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([[3, 2 ],[3, np.nan],[4,4]], columns=list('AB'))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *dropna()* function is used to remove missing values. Determine if rows or columns which contain missing values are removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Boolean mask\\n{df.isnull()} \\nDrop missing values')\n",
    "df.dropna()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop columns with too many missing values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_null = pd.DataFrame([[3, 2 ],[3, np.nan],[4,np.nan]], columns=list('AB'))\n",
    "df_null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_null.dropna(axis=1)\n",
    "# Drop columns using that limit\n",
    "limit = len(df_null) * 0.7 # How many non-NA values in each column/row? 2\n",
    "df_null.dropna(axis=1, thresh=limit) # columns having null values more than or equal to 70 percent are dropped from the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following example, the missing values in the 'B' column are replaced by the median value of that column. The values before and after replacement are shown for a subset of the data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['B'] =  df.B.fillna(df.B.median()) # median() exclude NA/null values when computing the result by default\n",
    "print(f\"Replace missing value in 'B' column by the median value \\n{df}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" style='color:black'>\n",
    "<b>Tip fillna</b>      \n",
    "We replace :\n",
    "\n",
    "* The mean  of the numerical column data is used to replace null values when the data is normally distributed\n",
    "* Median is used if the data comprised of outliers.\n",
    "* Mode is used when the data having more occurences of a particular value or more frequent value.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_city= pd.DataFrame({'city':[\"paris\", \"paris\", \"\", \"milan\"]})\n",
    "# df_city = df_city.replace({\"\": np.nan}) \n",
    "# df_city.info()\n",
    "# most_freq_city = df_city['city'].mode().iloc[0]\n",
    "# most_freq_city\n",
    "# df_city.city.fillna(most_freq_city) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There aren't null values in our dataset! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Are there null target values?   ',df_y.isnull().values.any())\n",
    "print('Are there null features values? ',df_X.isnull().values.any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some datasets, especially those obtained by merging multiple data sources, may contain duplicates or near duplicate instances. \n",
    "\n",
    "We  check for duplicate instances in the breast cancer dataset:\n",
    "* [*duplicated()*](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.duplicated.html): return boolean Series denoting duplicate rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([df_X,df_y], axis=1) \n",
    "dups = data.duplicated()\n",
    "print(f'Are there duplicate rows? = {dups.any()}')\n",
    "data = data.drop_duplicates() #  drop_duplicates return DataFrame with duplicate rows removed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There aren't duplicate instances! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling\n",
    "![](images/standardization.jpg)\n",
    "\n",
    "Dataset can contains different magnitude, units and range. Scaling is a method used to normalize the range of independent variables or features of data. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Why is data scaling important?\n",
    "\n",
    "# 3 people A,B,C\n",
    "df_cm_kg = pd.DataFrame({'height_cm':[180,180,100 ], 'weight_kg':[60,58.5,59]}, index=['A','B','C'])\n",
    "df_m_gr = pd.DataFrame({'height_m':[1.8,1.8,1.0], 'weight_gr':[60000,58500,59000]}, index=['A','B','C'])\n",
    "print(df_cm_kg)\n",
    "print(df_m_gr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function\n",
    "def draw_euclidean_distance(A,B,C,figsize=(15, 1), title='A,B,C weight and height (kg and cm)'):\n",
    "    \"\"\" Computes the distance  between the (A,B) and (A,C) points and plot the result \"\"\"\n",
    "    \n",
    "    # computes the distance  between the two points d = √(ΔEx^2 + ΔEy^2) = √((x1-x2)^2 + (y1-y2)^2)\n",
    "    distance_A_B = np.sqrt( ((A[0]-B[0])**2)+((A[1]-B[1])**2) )\n",
    "    distance_A_C = np.sqrt( ((A[0]-C[0])**2)+((A[1]-C[1])**2) )\n",
    "    \n",
    "    # plot result\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    ax.set(title=title, ylabel='weight', xlabel='height')\n",
    "    # draw A, B, C\n",
    "    ax.text(A[0],A[1],'A')\n",
    "    ax.text(B[0],B[1],'B')\n",
    "    ax.text(C[0],C[1],'C')\n",
    "\n",
    "    # draw the distance A , B\n",
    "    ax.plot((A[0],B[0]),(A[1],B[1]), color='green', linestyle='dashed', marker='o', markerfacecolor='blue',alpha=0.4)\n",
    "    ax.text((A[0]+B[0])/2,(A[1]+B[1])/2,f'd(A,B): {distance_A_B:.1f}')\n",
    "\n",
    "    # draw the distance A , C\n",
    "    ax.plot((A[0],C[0]),(A[1],C[1]), color='red', linestyle='dashed', marker='o', markerfacecolor='blue',alpha=0.4)\n",
    "    ax.text((A[0]+C[0])/2,(A[1]+C[1])/2,f'd(A,C): {distance_A_C:.1f}')\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw a scatter plot DF cm and kg\n",
    "# get A, B, C\n",
    "A = df_cm_kg.iloc[0,:]\n",
    "B = df_cm_kg.iloc[1,:]\n",
    "C = df_cm_kg.iloc[2,:]\n",
    "draw_euclidean_distance(A,B,C)\n",
    "\n",
    "\n",
    "# Draw a scatter plot DF m and gr\n",
    "# get A, B, C\n",
    "A = df_m_gr.iloc[0,:]\n",
    "B = df_m_gr.iloc[1,:]\n",
    "C = df_m_gr.iloc[2,:]\n",
    "draw_euclidean_distance(A,B,C,figsize=(1,6),title='A,B,C weight and height (gr and m)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfj = df_m_gr.join(df_cm_kg)\n",
    "# # In descriptive statistics, a box plot is a method of plotting numerical data through their quartiles.\n",
    "# dfj.plot(kind='box') #\n",
    "# plt.show()\n",
    "# dfj.apply(lambda x: (x-x.mean())).plot(kind='box')\n",
    "# plt.show()\n",
    "# dfj.apply(lambda x: (x-x.mean())/x.std()).plot(kind='box')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why is data scaling important?**\n",
    "1. Before comparing the features distribution, because differences between values of features are very high to observe on plot\n",
    "2. Many ML algorithm calculate the distance between two points by the Euclidean distance during the training phase. **The range of all features should be normalized so that each feature contributes proportionately to the final distance.**\n",
    "\n",
    "[*sklearn.preprocessing.StandardScaler*](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) standardizes the features by removing the mean and scaling to unit variance. The standard score (*z-scores*) of a sample x is calculated as:\n",
    "\n",
    "`z = (x - u) / s`\n",
    "\n",
    "where u is the mean of the data samples  and s is the standard deviation of the data samples.\n",
    "\n",
    "![](images/standardization.png) image from https://youtu.be/2tuBREK_mgE?t=165"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler #for Scaling the features\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_features =scaler.fit_transform(df_X.values)\n",
    "\n",
    "df_X_scaled = pd.DataFrame(scaled_features, index=df_X.index, columns=df_X.columns)\n",
    "df_X_scaled.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_X.apply(lambda x: (x - x.mean())/x.std())\n",
    "#scaler.inverse_transform(df_X_scaled.values)\n",
    "# df_X_scaled.values * np.sqrt(scaler.var_) + scaler.mean_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization and Trends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the number of samples in the two classes and the percentages.\n",
    "The plot below represents the class distribution of malignant and benign samples.\n",
    "\n",
    "Here we have 212 malignants (around 38% of the data) and 357 benign breast cancer masses (62%).\n",
    "\n",
    "Count plot visualization is done by using [seaborn.countplot](https://seaborn.pydata.org/generated/seaborn.countplot.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show the counts of observations using bars.\n",
    "ax = sns.countplot(df_y) # Returns the matplotlib Axes object\n",
    "\n",
    "\n",
    "for p in ax.patches: # gets shapes and draws annotations\n",
    "    height = p.get_height()\n",
    "    ax.text(p.get_x()+p.get_width()/2., height + 3,'{:1.2f}%'.format(height/len(df_y)*100), ha=\"center\") \n",
    "\n",
    "df_y.value_counts() # Return a Series containing counts of unique values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We plot features in 3 groups and each group includes 10 features to observe better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X_mean= df_X_scaled.iloc[:,:10]\n",
    "df_X_se= df_X_scaled.iloc[10:20]\n",
    "df_X_worst =df_X_scaled.iloc[20:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the dataset contain many variables, and relationship between each and every variable is to be analysed, a pair plot is used to visualize the data further. \n",
    "\n",
    "It shows the data as a collection of points.  The position of one variable in the same data row is matched with another variable’s value. Each value is a position on either the vertical or horizontal dimension indicates its correlation. \n",
    "\n",
    "**It allows both, distribution of single variables and relationships between two variables.** It is an effective method to identify trends for analysis.\n",
    "\n",
    "To implement pair plots in python seaborn is used and is made by using [seaborn.pairplot](https://seaborn.pydata.org/generated/seaborn.pairplot.html) function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.5)\n",
    "\n",
    "#Plot pairwise relationships in a dataset.\n",
    "sns.pairplot(pd.concat([df_X_mean,df_y], axis=1), hue='target')\n",
    "# sns.pairplot(pd.concat([df_X_se,df_y], axis=1),  hue='target')\n",
    "# sns.pairplot(pd.concat([df_X_worst,df_y], axis=1),  hue='target')\n",
    "# ax.set_yticklabels(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\" style='color:black'>\n",
    "<b>Observations</b>  \n",
    "\n",
    "1. The *radius*, *area* and *perimeter* characteristics are closely related.\n",
    "2. The benign tumors have smaller cell nuclei and  Binomial distribution (for *radius*, *area* and *perimeter*)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Violin plots** are used to represent comparison of a variable distribution (or sample distribution) across different \"categories\".\n",
    "[*seaboarn.violinplot*](https://seaborn.pydata.org/generated/seaborn.violinplot.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('whitegrid')\n",
    "plt.figure(figsize=(18,8))\n",
    "df_mean = pd.concat([df_X_mean, df_y], axis=1) \n",
    "\n",
    " \n",
    "# Unpivot the DataFrame from Wide to long format  |Target | Features| Value | [https://bit.ly/3j2Qj1O]\n",
    "data_for_violinplot = pd.melt(df_mean,id_vars=\"target\",var_name=\"features\", value_name='value')\n",
    "#violin plot\n",
    "sns.violinplot(x=\"features\", y=\"value\", hue=\"target\", data=data_for_violinplot, split=True, inner=\"quart\" )\n",
    "# sns.swarmplot(x=\"features\", y=\"value\", hue=\"target\", data=data_for_violinplot)\n",
    "plt.xticks(rotation=90) \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\" style='color:black'>\n",
    "<b>Observations</b>  \n",
    "\n",
    "1. The *mean concave points*, *mean concavity* and *mean area* have well seperated distribution on target variable (seems to be good predictors)\n",
    "2. The *mean fractal dimension*, *mean symmetry* and *mean smoothness* have similar distribution on target variable\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers Detection\n",
    "Outliers could be indicative of incorrect data, erroneous procedures or experimental areas where some theories may not be valid.\n",
    "\n",
    "Why finding outliers is important?\n",
    "* No Valid data (human error, erroneous procedures, instrumentation error, calibration error)\n",
    "* Data analysis and not only data cleaning (investigating the causes outliers in data)\n",
    "* Summarize data by statistics that represent the majority of the data\n",
    "* Train a model that generalizes to new data (remove outilers only in the training set)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### InterQuartile Range \n",
    "The Tukey’s boxplot is often used to pinpoint possible outliers. \n",
    "In this plot, a box is drawn from the ﬁrst quartile Q1= x[n_sample/4] to the third quartile Q3= x[3 n_sample/4] of the data (the data must be ordered from smallest to largest ). \n",
    "Points outside the interval [Q1 − 1.5IQR, Q3 + 1.5 IQR], called the fence (or whisker), are traditionally marked as outliers. \n",
    "\n",
    "IQR = Q3 − Q1.\n",
    "![](images/box_plot_ref_needed.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "sns.boxplot( data = df_X_scaled ) \n",
    "plt.xticks(rotation=90)  \n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$outlier(x)=\\begin{equation}\n",
    "\\left\\{ \n",
    "  \\begin{aligned}\n",
    "    0&   & (Q1 - 1.5 * IQR)\\leq x \\leq (Q3 + 1.5 * IQR) \\\\\n",
    "    1&   & Otherwise \n",
    "  \\end{aligned}\n",
    "  \\right.\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "where X is the parameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isOutlier(x):\n",
    "    # Return True if x < (Q1 - 1.5 * IQR) OR  x > (Q3 + 1.5 * IQR)\n",
    "    IQR = x.quantile(0.75)-x.quantile(0.25) # Q3 - Q1 \n",
    "    minimum = x.quantile(0.25) - 1.5 * IQR  # Q1 - 1.5 IQR\n",
    "    maximum = x.quantile(0.75) + 1.5 * IQR  # Q3 + 1.5 \n",
    "    return (x < minimum) | (x > maximum )\n",
    "\n",
    "\n",
    "df_X_clean_iqr = df_X[df_X.apply(lambda x: isOutlier(x)).any(axis=1)]\n",
    "# ~ is used to invert a boolean Series, https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#boolean-indexing\n",
    "\n",
    "# numpy.all Test whether all array elements along a given axis 1 evaluate to True. \n",
    "# we use ~isOutlier as dataframe mask to filter outliers\n",
    "# you can obtain the same result with: df_X_scaled[~df_X_scaled.apply(lambda x: isOutlier(x))].dropna()\n",
    "print(\"How many outliers?\",len(df_X)-len(df_X_clean_iqr))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the boxplot assumes symmetry because we add the same amount to Q3 as what we subtract from Q1.**At asymmetric distributions, the usual boxplot typically ﬂags many regular data points as outlying.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Z-Score\n",
    "People often use rules to detect outliers. The classical rule is based on the **z-scores** that we introduced before.\n",
    "**This technique assumes a Gaussian distribution of the data**. The outliers are the data points that are in the tails of the distribution and therefore far from the mean. \n",
    "\n",
    "$$outlier(x)=\\begin{equation}\n",
    "\\left\\{ \n",
    "  \\begin{aligned}\n",
    "    1&   &|X-\\hat{X}| \\geq 3 \\sigma \\\\\n",
    "    0&   &|X-\\hat{X}| \\leq 3 \\sigma \n",
    "  \\end{aligned}\n",
    "  \\right.\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "where **X** is the parameter values, $\\hat{X}$ is the mean of parameter, and **σ** is its standard deviation.\n",
    "\n",
    "More precisely, the rule ﬂags xi as **outlying if |zi| exceeds 3**, say.\n",
    "(This cutoff is based on the fact that when the data are normally distributed, 99.7% of the observations fall within 3 standard deviations around the mean).\n",
    "We would have considered outliers all the absolute values plotted over the value 3 on the y axis. \n",
    "\n",
    "<div class=\"alert alert-block alert-success\" style='color:black'>\n",
    "<b>Observations</b>  \n",
    "    <p>\n",
    "However, considering that our slightly skewed distributions and the availability of data is not very high (only 569 observations), it has been preferred to use a \"visual\" approach for detection (based on density of the points above a specific threshold) and detection of the outliers (very far from the mean). </p>\n",
    "<b>We notice a not very dense region of points above the 6 value.</b>\n",
    "    \n",
    "We suspect there is a mistake\n",
    " \n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X_outiler=df_X_scaled[df_X_scaled.apply(lambda x: np.abs(x) > 6).any(axis=1)] \n",
    "df_X_outiler # getting outilers data for discussing with domino experts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_outiler_noscaled = scaler.inverse_transform(df_X_outiler)\n",
    "df_X_outiler_noscaled = pd.DataFrame(data_outiler_noscaled, index=df_X_outiler.index, columns=df_X_outiler.columns)\n",
    "df_to_check = df_X_outiler_noscaled.join(df_y)\n",
    "df_to_check.to_csv(\"data/WBCD_outliers_for_doctors.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Outlier detection is important in medical applications* as well as many other applications that requires concentrating on uncommon activities. **It can identify potential patient management errors or problems with the instrumentation.**\n",
    "\n",
    "**Let's assume that the doctors confirm to us that the 12 outliers we found\n",
    "they are invalid data that we can then eliminate from our analysis.**\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\" style='color:black'>\n",
    "<b>NOTE:</b>\n",
    "Before removing them we should discuss with domino experts to understand why these points are not valid (for example, the measuring equipment failed, the measurement method was unreliable for some reason, there were contaminants, etc ...).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X_clean=df_X_scaled[df_X_scaled.apply(lambda x: np.abs(x) < 6).all(axis=1)] \n",
    "print(\"How many outliers?\",len(df_X_scaled)-len(df_X_clean))\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "sns.boxplot( data = df_X_clean )\n",
    "plt.xticks(rotation=90)  \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<marquee style='width: 30%; color: red;'><b>Important! Golden Rules</b></marquee>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it’s important to investigate the nature of the outlier before deciding whether to drop it or not.\n",
    "* <div class=\"alert alert-block alert-warning\" style='color:black'> <b>DROP</b> If it is obvious that the outlier is due to incorrectly entered or measured data, you should drop the outlier.</div>\n",
    "\n",
    "For example a year field with a '9999' value. This is can be a human, instrument error or calibration error.\n",
    "\n",
    "* <div class=\"alert alert-block alert-warning\" style='color:black'><b>DROP AND EXPLAIN WHY</b> More commonly, the outlier can affect results. In this situation, it is not legitimate to simply drop the outlier. You may run the analysis both with and without it.</div>\n",
    "\n",
    "For example, you can remove the outliers (**only in the training dataset used to build the model**) and compare the result with the model obtained from raw data. We will talk about predictive models, training sets and testing sets in the next lesson.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw some simple univariate outlier detection techniques,  you can find more details in the [APPENDIX](#APPENDIX)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation\n",
    "A correlation matrix is a table showing correlation coefficients between variables. Each cell in the table shows the correlation between two variables. A correlation matrix is used to summarize data and as a diagnostic for advanced analyses. [seaborn.heatmap](https://seaborn.pydata.org/generated/seaborn.heatmap.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=0.8)\n",
    "plt.figure(figsize=(16,9))\n",
    "sns.heatmap( df_X_clean.corr(),  square = False, annot=True, fmt=\".2f\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discovering structure in heatmap data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can the correlation matrix as a hierarchically-clustered heatmap in order to find easly the pattern, for example the variables highly correlate with each other. [seaborn.clustermap](https://seaborn.pydata.org/generated/seaborn.clustermap.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.0)\n",
    "# Correlation is a term that is a measure of the strength of a linear relationship between two quantitative variables.\n",
    "corr_matrix = df_X_clean.corr()\n",
    "sns.clustermap(corr_matrix,annot=True,fmt=\".2f\",figsize=(20,14)) # Plot the correlation matrix as a hierarchically-clustered heatmap.\n",
    "plt.title(\"Correlation Between Features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\" style='color:black'>\n",
    "<b>Observations</b>  \n",
    " Radius, area and perimeter (mean, the worst, and the standard error ) have stronge positive correlation. \n",
    "</div>\n",
    "\n",
    "[seaborn.jointplot](http://seaborn.pydata.org/generated/seaborn.jointplot.html) : plotting a bivariate relationship and distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats #  library of statistical functions (we'll use to compute pearson correlation)\n",
    "\n",
    "# Plot correlation between 2 features and distribution\n",
    "sns.set(style=\"white\")\n",
    "\n",
    "# seaborn.jointplot draws a plot of two variables with bivariate and univariate graphs\n",
    "ax = sns.jointplot(df_X_clean.loc[:,'mean area'], \n",
    "              df_X_clean.loc[:,'mean radius'], \n",
    "              kind=\"reg\" )\n",
    "\n",
    "ax.ax_joint.text(-1,5, f\"Pearsonr's correlation: {stats.pearsonr(df_X_clean.loc[:,'mean area'], df_X_clean.loc[:,'mean radius'])[0]:.2f}\")\n",
    "\n",
    "# sns.jointplot(df_X_clean.loc[:,'mean radius'], \n",
    "#               df_X_clean.loc[:,'mean area'], \n",
    "#               kind=\"reg\")\n",
    "# sns.jointplot(df_X_clean.loc[:,'mean area'], \n",
    "#               df_X_clean.loc[:,'mean perimeter'], \n",
    "#               kind=\"reg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing multi colinearity\n",
    "<div class=\"alert alert-block alert-warning\" style='color:black'>\n",
    "Two highly correlated input variables probably carry similar information. \n",
    "We can remove multi colinearity, it means the columns are dependenig on each other so we should avoid it because what is the use of using same column twice.\n",
    "</div>\n",
    "\n",
    "So, we can drop _worst radius,worst area,worst perimeter,area error, area error, perimeter error, radius error, mean perimeter, mean radius_ columns (they have a strong correlation with  _mean area_) and leave only _mean area_.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get original scale\n",
    "data_noscaled = scaler.inverse_transform(df_X_clean)\n",
    "df_X_clean_noscaled = pd.DataFrame(data_noscaled, index=df_X_clean.index, columns=df_X_clean.columns)\n",
    "## drop multicolinearity variables\n",
    "df_X_clean_noscaled = df_X_clean_noscaled.drop(['worst radius','worst area','worst perimeter','area error', 'area error', 'perimeter error', 'radius error', 'mean perimeter', 'mean radius'],axis=1)\n",
    "df_X_clean_noscaled.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## saving the preprocessed data on CSV file\n",
    "df_X_y_clean = df_X_clean_noscaled.join(df_y)\n",
    "df_X_y_clean.to_csv(\"data/WBCD_preprocessed.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation ‘among the predictors’ is a problem to be rectified to be able to come up with a reliable model.\n",
    "Another measure that is commonly used to help diagnose multicollinearity is presented in the [APPENDIX](#APPENDIX)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction\n",
    "Too many variables can cause such problems below:\n",
    "* Increased computational cost\n",
    "* Too complex visualization problems\n",
    "* Decrease efficiency by including variables that have no effect on the analysis\n",
    "* Make data interpretation difficult\n",
    "\n",
    "Datasets often have many (hundreds or even thousands) features and dropping redundant variables is not enough to reduce the dimensionality of the dataset.\n",
    "\n",
    "\n",
    "more details and other dimensionality reduction algorithms are listed in the [APPENDIX](#APPENDIX)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis (PCA)\n",
    "[Principal Component Analysis](https://en.wikipedia.org/wiki/Principal_component_analysis) is fundamentally a dimensionality reduction algorithm, but it can also be useful as a tool for visualization, for noise filtering, for feature extraction and engineering, and much more.\n",
    "\n",
    "**PCA reduces the dimensionality of a dataset, while preserving as much ‘variability’ (i.e.statistical information) as possible.\n",
    "It converts a set of possibly correlated predictors into a set of linearly uncorrelated variables**\n",
    "(using [Singular Value Decomposition](https://en.wikipedia.org/wiki/Singular_value_decomposition) of the data to project it to a lower dimensional space). \n",
    "\n",
    "<!-- PCA is based on two basic considerations: \n",
    "* high correlation between variables indicates redundancy in the data; \n",
    "* the most important variables express higher variance. \n",
    "\n",
    "Based on these considerations, the model simplifies the complexity of the variables; -->\n",
    "\n",
    "**PCA tries to explain the covariance structure of the data by means of a (hopefully small) number of components `(PC1, PC2,..,PCn)=PCA(X1,X2,...,Xn)`.These components are linear combinations of the original variables**, and often allow for an interpretation and a better understanding of the different sources ofvariation. \n",
    "\n",
    "<!-- ![](img/pca.png)  -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/g-Hb26agBFg?mute=1&start=49&end=120&cc_load_policy=1&fs=0&iv_load_policy=3&rel=1\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before applying PCA, we scale our data such that each feature has unit variance. This is necessary because fitting algorithms highly depend on the scaling of the features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler() # scaling data before PCA\n",
    "scaled_features =scaler.fit_transform(df_X_clean_noscaled.values)\n",
    "df_X_clean_scaled = pd.DataFrame(scaled_features, index=df_X_clean_noscaled.index, columns=df_X_clean_noscaled.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[*sklearn.decomposition.PCA*](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA # Principal Component Analysis module\n",
    "\n",
    "pca2d = PCA(n_components=2)  \n",
    "pc = pca2d.fit_transform(df_X_clean_scaled.values) # computes PCA components and trasforms original data\n",
    "\n",
    "pc_df = pd.DataFrame(data = pc , columns = ['PC1', 'PC2'])\n",
    "pc_df['target'] = df_X_y_clean.target.values\n",
    "\n",
    "print(pc_df.shape)\n",
    "pc_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we can create a visualization of our dataset\n",
    "plt.figure(figsize=(12, 7))\n",
    "\n",
    "sns.scatterplot(\n",
    "    data=pc_df, x=\"PC1\", y=\"PC2\", hue=\"target\"\n",
    ")\n",
    "plt.title('PCA Scatter Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\" style='color:black'>\n",
    "<b>Observations</b>  \n",
    "it can be deduced that the data are linearly separable and therefore a linear model could be an good solution to solve a classification problem. \n",
    "</div>\n",
    "\n",
    "\n",
    "*In the PCA, the next question is “how many principal components are we going to choose for our new feature subspace?*\n",
    "\n",
    "A number of principal components must be considered such that they **take into account a sufficiently high percentage of total variance** (at least 70%, for example, when stop increasing substantially). When defining the minimum percentage of acceptable variance, the number of original variables should be taken into account, so that as the number of variables increases, a lower percentage of explained variance may be accepted.\n",
    "\n",
    "**The explained variance tells us how much information (variance) can be attributed to each of the principal components**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca2d.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca10 = PCA(n_components=10)\n",
    "pca10.fit(df_X_clean_scaled.values)\n",
    "\n",
    "plt.figure(1, figsize=(14, 7))\n",
    "plt.bar(range(1,11,1), pca10.explained_variance_ratio_, alpha=0.5, align='center',\n",
    "        label='individual explained variance')\n",
    "plt.step(range(1,11,1),pca10.explained_variance_ratio_.cumsum(), where='mid',\n",
    "         label='cumulative explained variance')\n",
    "plt.ylabel('Explained variance ratio')\n",
    "plt.xlabel('Principal components')\n",
    "plt.title(\"Around 95% of variance is explained by the Fisrt 10 components \");\n",
    "plt.legend(loc='best')\n",
    "plt.axhline(y=0.7, color='r', linestyle='-') # 70% of  explained variance\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* PC1 describes most of the variability in the data, PC2 adds the next big contribution, and so on. In the end, the last PCs do not bring much more\n",
    "information to describe the data.\n",
    "* Thus, to describe the data we could use only the top m < n (i.e., i;, PC1, ⋯ PCn) components with little - if any - loss of information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\" style='color:black'>\n",
    "<b>Observations</b>   A good number of principal component can be 6 because the total explained variance stops increasing substantially or I can choose 3 with the explained variance of 74% is enough (and the rest is noise)  </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca3d = PCA(n_components=3)\n",
    "pc3 = pca3d.fit_transform(df_X_clean_scaled.values)\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D # 3D scatter plot\n",
    "fig = plt.figure(figsize=(12,7))\n",
    "ax = Axes3D(fig) \n",
    "\n",
    "cmap = {'benign':'orange','malignant':'blue'}\n",
    "ax.scatter(pc3[:,0], pc3[:,1], pc3[:,2], c=[cmap[c] for c in  df_X_y_clean.target.values],\n",
    "           marker='o', s=20)\n",
    "\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "ax.set_zlabel('PC3')\n",
    "ax.view_init(30,-110)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find more dimensionality reduction techniques in the [APPENDIX](#APPENDIX)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "Discover hidden structures in unlabeled data (unsupervised).\n",
    "Clustering identifies a finite set of groups (clusters) C1, C2 ⋯ , Ck in the dataset such that:\n",
    "* Objects within the same cluster Ci shall be as similar as possible\n",
    "* Objects of different clusters Ci, Cj (i ≠ j) shall be as dissimilar as possible\n",
    "\n",
    "Clustering algorithms can be categorized based on their cluster model.\n",
    "\n",
    "## K-Means\n",
    "**K-means clustering aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean.**\n",
    "The means are commonly called the cluster “centroids”; \n",
    "![image](images/cluster1.jpg)\n",
    "**K-Means Algorithm:**\n",
    "1. Guess some cluster centers\n",
    "2. Repeat until converged\n",
    "    - *Find closest centroid*: assign points to the nearest cluster center\n",
    "    - *Update centroid*: set the cluster centers to the mean\n",
    "    \n",
    "[Visualizing K-Means Clustering](https://stanford.edu/class/engr108/visualizations/kmeans/kmeans.html)\n",
    "\n",
    "<img src='images/1_rwYaxuY-jeiVXH0fyqC_oA.gif'>\n",
    "\n",
    "[sklearn.cluster.KMeans](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html): The KMeans estimator class in scikit-learn perform  K-Means Clustering.\n",
    "\n",
    "<!-- 1. Getting  K values as input which is the number of clusters or centroids\n",
    "2. Selecting random centroids for each cluster\n",
    "3. Assigning each data point to its closest centroid (Euclidean distance)\n",
    "4. Adjusting the centroid for the newly formed cluster in step 3\n",
    "5. Repeating step 3 and 4 till all the data points are perfectly organised within a cluster space -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "X = df_X_clean_scaled.values\n",
    "\n",
    " # k-means++ is an algorithm for choosing a good initial centroids (far away from each other). https://en.wikipedia.org/wiki/K-means%2B%2B\n",
    "kmeans = KMeans(n_clusters = 2, init = 'k-means++', random_state = 42)\n",
    "kmeans.fit(X)\n",
    "\n",
    "pd.Series(kmeans.labels_, name='cluster')"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Good Number Of Clusters\n",
    "Each cluster is formed by calculating and comparing the distances of data points within a cluster to its centroid. An ideal way to figure out the right number of clusters would be to calculate the Within-Cluster-Sum-of-Squares (WCSS or SSE). \n",
    "\n",
    "WCSS is the sum of squares of the distances of each data point in all clusters to their respective centroids.\n",
    "\n",
    "![image.png](images/1_zlZOSJB_DISgUxb06QwISw.png)\n",
    "\n",
    "We can find the good value for K using an Elbow point graph. We randomly initialise the K-Means algorithm for a range of K values and will plot it against the WCSS for each K value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_X_clean_scaled.values\n",
    "from sklearn.cluster import KMeans\n",
    "wcss = []\n",
    "n_clusters = 6\n",
    "for i in range(1, n_clusters+1):\n",
    "    kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42) # k-means++ is an algorithm for choosing a good initial centroids. \n",
    "    kmeans.fit(X)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "    \n",
    "plt.plot(range(1, n_clusters+1), wcss)\n",
    "plt.title('The Elbow Method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.axvline(x=2, color='r', linestyle='-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\" style='color:black'>\n",
    "<b>Observations</b>   The “elbow” (the point of inflection on the curve) is a good indication that the underlying model fits best at that point.\n",
    "For the above-given graph, the optimum value for K would be 2 </div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-means++ is an algorithm for choosing the initial centroids. New centroid is a far point from the other centroids\n",
    "kmeans = KMeans(n_clusters = 2, init = 'k-means++', random_state = 42)\n",
    "kmeans.fit(X)\n",
    "\n",
    "centroid = pca2d.transform(kmeans.cluster_centers_)\n",
    "centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_X_scaled_y_clean = df_X_clean_scaled.join(df_y)\n",
    "\n",
    "# get features vector mean for Benign and Malignant \n",
    "mean_pca_M = pca2d.transform([df_X_clean_scaled [df_y == 'malignant'].mean().values])\n",
    "mean_pca_B = pca2d.transform([df_X_clean_scaled [df_y == 'benign'].mean().values])\n",
    "\n",
    "# or\n",
    "# mean_pca_M = pca2d.transform([df_X_scaled_y_clean[ df_X_scaled_y_clean['target']=='malignant'].iloc[:,:-1].mean().values])\n",
    "# mean_pca_B = pca2d.transform([df_X_scaled_y_clean[ df_X_scaled_y_clean['target']=='benign'].iloc[:,:-1].mean().values])\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "cmap = {'benign':'orange','malignant':'blue'}\n",
    "# plots kemans clusters  \n",
    "cmap = {1:'orange',0:'blue'}\n",
    "plt.scatter(pc_df.iloc[:,0], pc_df. iloc[:,1], c=[cmap[c] for c in  kmeans.labels_] )\n",
    "# plots kemans centroids  \n",
    "plt.scatter(centroid[:,0],centroid[:,1], c='black', marker='x',s=100,\n",
    "            label=\"cluster centroids\")  \n",
    "# plots features vector mean for Benign and Malignant \n",
    "plt.scatter([mean_pca_M[:,0], mean_pca_B[:,0]],[mean_pca_M[:,1], mean_pca_B[:,1]], c='red',marker='+',s=50,\n",
    "            label=\"mean target class (b,m)\") \n",
    "\n",
    "plt.legend()\n",
    "plt.title('PCA Scatter Plot (green cluster 0, red cluster 1)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\" style='color:black'>\n",
    "<b>Observations</b>  : It is interesting to see how the centroids of the clusters fall very close to the average values of the two classes of tumors (benign and malignant). Therefore it is possible to note that, if we did not have a labeled dataset (with well-defined classes B and M) we would still be able to determine (with good probability) the class of belonging of the dataset elements, through an unsupervised clustering process .</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hierarchical clustering is presented in the [APPENDIX](#APPENDIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APPENDIX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This appendix includes further information on the course topics but **will not be exam topics**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory data analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing values\n",
    "[KNNImputer](https://scikit-learn.org/stable/modules/generated/sklearn.impute.KNNImputer.html) The k nearest neighbors algorithm can be used for imputing missing data by finding the k closest neighbors to the observation with missing data and then imputing them based on the the non-missing values in the neighbors.\n",
    "\n",
    "Each sample’s missing values are imputed using the mean value from n_neighbors nearest neighbors found in the training set. Two samples are close if the features that neither is missing are close."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.impute import KNNImputer\n",
    "X = [[1, 2, np.nan], [3, 4, 3], [np.nan, 6, 5], [8, 8, 7]]\n",
    "imputer = KNNImputer(n_neighbors=2)\n",
    "imputer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing: Convert Skewed data to Normal Distribution\n",
    "When it comes to skewed distributions, the most common\n",
    "response is to transform the data\n",
    "Generally, the most common type of skewness is\n",
    "right-skewness\n",
    "Consequently, the most common type of transformation is the\n",
    "log transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import skewnorm\n",
    "df_skewed = pd.DataFrame({'car_crash': skewnorm.rvs(10, size=100)+1}) # positive skewed data\n",
    "sns.distplot(df_skewed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "## log  transformation: can't be applied to zero or negative values\n",
    "sns.distplot(np.log(df_skewed))\n",
    "##square root transformation\n",
    "sns.distplot(np.sqrt(df_skewed))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring dataset: Decision Tree\n",
    "Although still under data exploration, we can get useful information by training a Decision Tree from all characteristics. We will explain the Decision Tree model better in the second part of this lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "X = df_X_clean_scaled.values\n",
    "y = df_X_y_clean.target.values \n",
    "\n",
    "clf = tree.DecisionTreeClassifier(max_depth=3,min_samples_leaf=8)  \n",
    "clf.fit(X, y)\n",
    "\n",
    "fig = plt.figure(figsize=(25,20))\n",
    "out = tree.plot_tree(clf, \n",
    "                   feature_names=df_X_clean_scaled.columns,  \n",
    "                   class_names=['Malignant','Benign'],\n",
    "                   filled=True)\n",
    "for o in out: \n",
    "    arrow = o.arrow_patch\n",
    "    if arrow is not None:\n",
    "        arrow.set_edgecolor('red')\n",
    "        arrow.set_linewidth(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Leys C, Delacre M, Mora YL, Lakens D, Ley C. How to Classify, Detect, and Manage Univariate and Multivariate Outliers, With Emphasis on Pre-Registration. International Review of Social Psychology. 2019;32(1):5. DOI: http://doi.org/10.5334/irsp.289](https://www.rips-irsp.com/articles/10.5334/irsp.289). \n",
    "2. [Rousseeuw, P.J. and Hubert, M. (2018), Anomaly detection by robust statistics. WIREs Data Mining Knowl Discov, 8: e1236.](https://doi.org/10.1002/widm.1236)\n",
    "\n",
    "They suggest the use of the median absolute deviation (MAD) to detect univariate outliers. \n",
    "Again, the idea is conceptually similar to computed z-scores: for each value, subtract the median from it, and divide by the median of the absolute deviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mad(x):\n",
    "    return 1.483 * np.median(np.abs(x- np.median(x)))\n",
    "\n",
    "df_X_clean_mad = df_X[df_X.apply(lambda x: (x - np.median(x))/mad(x) <= 3.0).all(axis=1)]\n",
    "print(\"How many outliers?\",len(df_X)-len(df_X_clean_mad))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate\n",
    "[Sklearn includes different advanced outlier detection methods based on machine learning (ML)](https://scikit-learn.org/stable/modules/outlier_detection.html) can handle correlated multivariate dataset, detect abnormalities within them, and do not assume a normal distributions of the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another measure that is commonly used to help diagnose multicollinearity is the **variance inflation factor (VIF)**.\n",
    "Although correlation matrix and scatter plots can also be used to find multicollinearity, their findings only show the bivariate relationship between the independent variables. **VIF is preferred as it can show the correlation of a variable with a group of other variables.**\n",
    "\n",
    "\n",
    "VIF measures how much of the variation in one variable is explained by the other variable. This is done by running a regression using one of the correlated x variables as the dependent variable against the other variables as predictor variables.\n",
    "![](images/vip.png)\n",
    "\n",
    "Use the following guidelines to interpret the VIF:\n",
    "\n",
    "|VIF| Status of predictors|\n",
    "|------|------|\n",
    "|VIF = 1|Not correlated|\n",
    "|1 < VIF < 5|Moderately correlated|\n",
    "|VIF > 5 to 10|Highly correlated|\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor \n",
    "# VIF dataframe \n",
    "vif_data = pd.DataFrame() \n",
    "vif_data[\"feature\"] = df_X_scaled.columns\n",
    "\n",
    "# calculating VIF for each feature \n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(df_X_scaled.values, i) \n",
    "                          for i in range(len(df_X_scaled.columns))] \n",
    "  \n",
    "print(vif_data.sort_values(by='VIF', ascending=False ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " _mean radius_ is  highly correled to the other variable. Dropping variables should be an iterative process starting with the variable having the largest VIF. \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Dimensionality Reduction \n",
    "### Feature Selection using Scikit Learn\n",
    "Feature selection works by selecting the best features based on univariate statistical tests. It can be seen as a preprocessing step to an estimator. \n",
    "* [SelectKBest()](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html) removes all but the k highest scoring features\n",
    "[The Chi-square (χ2) test](https://en.wikipedia.org/wiki/Chi-squared_test) is used to examine whether observed data fits with expected data.\n",
    "\n",
    "It is a test of independence and it is used to determine if there is a significant relationship between two variables. **Recall that the chi-square test measures dependence between stochastic variables**, so using this function “weeds out” the features that are the most likely to be independent of class and therefore irrelevant for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest,chi2\n",
    "def selector(X, y, k=12):\n",
    "    \n",
    "    \"\"\"The function receive features and labels (X, y) and a target number to select features (k)\n",
    "    base on the chi-square (χ2) and return a new dataframe wiht k best features\"\"\"\n",
    "    \n",
    "    selector = SelectKBest(chi2, k)\n",
    "    \n",
    "    best_X = selector.fit_transform(X, y)\n",
    "    \n",
    "    return pd.DataFrame(best_X, columns=X.columns[selector.get_support()])\n",
    "\n",
    "best_X = selector(df_X_clean_noscaled, df_X_y_clean.target, 5)\n",
    "\n",
    "best_X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### PCA\n",
    "PCA is a statistical procedure that (orthogonally) transforms the original n coordinates of a data set into a new set of n coordinates,\n",
    "called principal components.\n",
    "\n",
    "`(PC1, PC2,..,PCn)=PCA(X1,X2,...,Xn)`\n",
    "\n",
    "The first principal component PC1 follows the direction (eigenvector) of the largest possible variance (largest eigenvalue of the covariance matrix) in the data.\n",
    "\n",
    "Each succeeding component PCk follows the direction of the next largest possible variance under the constraint that it is orthogonal to (i.e., uncorrelated with) the preceding components\n",
    "\n",
    "In a nutshell, The principal components  are eigenvectors of the data's covariance matrix. Thus, the principal components are often computed by eigendecomposition of the data covariance matrix or singular value decomposition of the data matrix. \n",
    "![](images/pca_2.jpg)\n",
    "\n",
    "### Visualize Loadings\n",
    "It is also possible to visualize loadings using shapes, and use annotations to indicate which feature a certain loading original belong to. \n",
    "\n",
    "* PCA loading plot which shows how strongly each characteristic influences a principal component.\n",
    "\n",
    "For more details about the linear algebra behind eigenvectors and loadings, see this [Q&A thread](https://stats.stackexchange.com/questions/143905/loadings-vs-eigenvectors-in-pca-when-to-use-one-or-another).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import biplot_pca\n",
    "biplot_pca( pc_df, pca2d, df_X_clean_scaled.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuing the exploratory process aimed at understanding how to simplify the dataset, without losing relevant information, the following processes were applied:  UMAP and TSNE , which are techniques for reducing complexity; in particular:\n",
    "### TSNE and UMAP\n",
    "* [TSNE (T-distributed Stochastic Neighbor Embedding)](https://lvdmaaten.github.io/publications/papers/JMLR_2008.pdf) is a non-linear dimensionality reduction technique that is particularly suited to reducing the complexity of multidimensional datasets in a two- or three-dimensional model. **It is better than PCA, but it is computationally expensive**\n",
    "* [UMAP (Uniform Manifold Approximation and Projection)](https://arxiv.org/abs/1802.03426) is a dimension reduction technique that can be used for visualisation **similarly to t-SNE, but with superior run time performance.**\n",
    "\n",
    "By reducing the dimension in a way that preserves as much of the structure of the data as possible we can get a visualisable representation of the data allowing us to “see” the data and its structure and begin to get some intuition about the data itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "#  umap-learn for umap dimension reduction\n",
    "!{sys.executable} -m pip install pip -U\n",
    "!{sys.executable} -m pip install umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP\n",
    "from sklearn.manifold import TSNE\n",
    "import time \n",
    "X = df_X_clean_scaled.values\n",
    "y = df_X_y_clean.target.values \n",
    "\n",
    "print(f\"[{time.asctime(time.localtime())}] Starting\")\n",
    "# Invoke the TSNE method\n",
    "tsne_results = TSNE(n_components=2, verbose=0, perplexity=40, n_iter=2000).fit_transform(X)\n",
    "print(f\"[{time.asctime(time.localtime())}] Completed TSNE\")\n",
    "# Invoke the UMAP method\n",
    "reducer = UMAP().fit_transform(X) \n",
    "print(f\"[{time.asctime(time.localtime())}] Completed UMAP\")\n",
    "# Invoke the PCA method\n",
    "pc = PCA(n_components=2).fit_transform(X)\n",
    "print(f\"[{time.asctime(time.localtime())}] Completed PCA\")\n",
    "\n",
    "# Plot the TSNE and PCA visuals side-by-side\n",
    "cmap = {'benign':'orange','malignant':'blue'}\n",
    "fig = plt.figure(figsize = (15,9))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.title('TSNE Scatter Plot')\n",
    "plt.scatter(tsne_results[:,0], tsne_results[:,1],  c =[cmap[x] for x in y] , alpha=0.75)\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.title('UMAP Plot')\n",
    "plt.scatter( reducer[:,0], reducer[:,1], c =[cmap[x] for x in y] ,alpha=0.75)\n",
    "      \n",
    "plt.subplot(2, 2, 3)\n",
    "plt.title('PCA Plot')\n",
    "plt.scatter( pc[:,0], pc[:,1], c =[cmap[x] for x in y] ,alpha=0.75)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "Clustering techniques can be divided into two approaches: partition (like Kmeans) and hierarchical. The next section introduces hierarchical clustering, but before we will introduce a well-known clustering quality measure.\n",
    "\n",
    "We suggest [Amit Saxena et al., A review of clustering techniques and developments, 2017](https://doi.org/10.1016/j.neucom.2017.06.053) paper for comprehensive study on clustering\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Silhouette: Clustering quality measure\n",
    "Silhouette-Coefficient measures the quality of clustering\n",
    "* a(x): distance of object x to its cluster representative\n",
    "* b(x): distance of object x to the representative of the second-best cluster\n",
    "* Silhouette s(x) of x\n",
    "![image.png](images/silhouette.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computes silhouette coefficients for each point, and **average it out for all the samples to get the silhouette score**.\n",
    "\n",
    "**The silhouette value is a measure of how similar an object is to its own cluster (cohesion) compared to other clusters (separation).**\n",
    "The value of the silhouette ranges between [1, -1], where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters.\n",
    "\n",
    "If most objects have a high value, then the clustering configuration is appropriate. If many points have a low or negative value, then the clustering configuration may have too many or too few clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "# The silhouette_score gives the average value for all the samples.\n",
    "silhouette_score(X,  kmeans.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import kmeeans_silhouette_analysis\n",
    "sns.set_style('darkgrid') \n",
    "kmeeans_silhouette_analysis(X, range(2, n_clusters+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hierarchical clustering is presented in the [APPENDIX](#APPENDIX). The aspects to look out for in Silhouette plots are cluster scores below the average silhouette score, wide fluctuations in the size of the clusters, and also the thickness of the silhouette plot.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\" style='color:black'>\n",
    "<b>Observations</b>  \n",
    "\n",
    "    \n",
    "* In our example, the analysis of the silhouette is used to choose an optimal value for the number of clusters. The silhouette plot shows that a n_clusters value of 5 and 6 is not good because they have the silhouette score lower than average scores, many negative values and also large fluctuations in the size of the silhouette plot. **From the analysis of the silhouette, a good number of k clusters appears to be 2 since it confirms what has already been expressed by the elbow method.**\n",
    "\n",
    "* Both Elbow method / SSE Plot and Silhouette method can be used interchangeably based on the details presented by the plots. It may be good idea to use both the plots just to make sure that you select most optimal number of clusters.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical clustering\n",
    "\n",
    "In data mining and statistics, hierarchical clustering (also called hierarchical cluster analysis or HCA) is a method of cluster analysis which seeks to build a hierarchy of clusters. [wiki](https://en.wikipedia.org/wiki/Hierarchical_clustering)\n",
    "**Construction of a hierarchy of clusters (dendrogram) by merging/separating clusters with minimum/maximum distance**\n",
    "* The agglomerative follows the bottom-up approach, which builds up clusters starting with single object and then merging these atomic clusters into larger and larger clusters using a linkage function, until all of the objects are finally lying in a single cluster or otherwise until certain termination conditions are satisfied**. \n",
    "* The divisive hierarchical clustering follows the top-down approach, which breaks up cluster containing all objects into smaller clusters, until each object forms a cluster on its own or until it satisfies certain termination conditions. The hierarchical methods usually lead to formation of dendrograms as shown.\n",
    "\n",
    "<img src='images/hclustering.jpg' />\n",
    "\n",
    "**Base Algorithm**\n",
    "1. Form initial clusters consisting of a single object, and compute the distance between each pair of clusters.\n",
    "2. Merge the two clusters having minimum distance.\n",
    "3. Calculate the distance between the new cluster and all other clusters.\n",
    "4. If there is only one cluster containing all objects: Stop, otherwise go to step 2. \n",
    "\n",
    "<img src='images/hclustering_measures.jpg' width='700' />\n",
    "\n",
    "We used the Ward linkage method, it has the highest performance in most situations, except when there were verylarge differences among cluster sizes. \n",
    "[A COMPARISON OF HIERARCHICAL METHODS FOR CLUSTERING FUNCTIONAL DATA](https://people.stat.sc.edu/Hitchcock/compare_hier_fda.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import plot_dendrograms\n",
    "plot_dendrograms(X) # Ward dendrom has well separated and compact clusters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage,median  # linkage analysis and dendrogram for visualization\n",
    "from scipy.cluster.hierarchy import fcluster  # simple clustering\n",
    "from scipy.spatial.distance import pdist, squareform # metric\n",
    "\n",
    "sns.set_style('whitegrid') \n",
    "X = df_X_clean_scaled.values\n",
    "#Perform hierarchical/agglomerative clustering using Ward method. \n",
    "# Ward = Similarity of two clusters is based on the increase in squared error when two clusters are merged\n",
    "Z = linkage(X, method='ward', metric='euclidean') \n",
    "\n",
    "plt.figure(figsize=(15, 7))\n",
    "# plots dendograms\n",
    "dendrogram(\n",
    "    Z,    \n",
    "    leaf_rotation=90.,\n",
    "    leaf_font_size=11.,\n",
    "    show_contracted=True,\n",
    "    distance_sort='descending',\n",
    "    truncate_mode = 'lastp',\n",
    "    p=50\n",
    ")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* horizontal lines are cluster merges\n",
    "* vertical lines tell you which clusters/labels were part of merge forming that new cluster\n",
    "* heights of the horizontal lines tell you about the distance that needed to be \"bridged\" to form the new cluster\n",
    "* a huge jump in distance is typically what we're interested to find the optimal number of clusters. \n",
    "\n",
    "The dendrogram function with Ward method divides the data into 2 groups (it cuts to 70% of the maximum length) by default \n",
    "https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.cluster.hierarchy.dendrogram.html\n",
    "    \n",
    "The 2 clusters are well separated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some automated Cut-Off selection methods [but they are not very reliable](https://joernhees.de/blog/2015/08/26/scipy-hierarchical-clustering-and-dendrogram-tutorial/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting clusters from dendogram\n",
    "k=2\n",
    "clusters = fcluster(Z, k, criterion='maxclust')\n",
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  features vector mean for Benign and Malignant \n",
    "mean_pca_M = pca2d.transform([df_X_clean_scaled [df_y == 'malignant'].mean().values])\n",
    "mean_pca_B = pca2d.transform([df_X_clean_scaled [df_y == 'benign'].mean().values])\n",
    "\n",
    "# features vector mean for clusters\n",
    "mean_pca_c1 = pca2d.transform([df_X_clean_scaled[clusters == 1].mean().values])\n",
    "mean_pca_c2 = pca2d.transform([df_X_clean_scaled[clusters == 2].mean().values])\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "# plot points with cluster dependent colors\n",
    "plt.scatter(pc_df.iloc[:,0], pc_df.iloc[:,1],c= clusters, cmap='prism', s=20)  \n",
    "# plots hcluster centroids  \n",
    "plt.scatter([mean_pca_c2[:,0], mean_pca_c1[:,0]],[mean_pca_c2[:,1], mean_pca_c1[:,1]], c='black', cmap='prism',marker='x',s=100,\n",
    "            label=\"cluster centroids\")  \n",
    "# plots features vector mean for Benign and Malignant \n",
    "plt.scatter([mean_pca_M[:,0], mean_pca_B[:,0]],[mean_pca_M[:,1], mean_pca_B[:,1]], c='blue', cmap='prism',marker='+',s=50,\n",
    "            label=\"mean target class (b,m)\") \n",
    "\n",
    "plt.legend()\n",
    "plt.title('PCA Scatter Plot (green cluster 0, red cluster 1)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison between real centroids and hierarchical clustering centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_score(X,  clusters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_ipdpp_gssi2021",
   "language": "python",
   "name": "env_ipdpp_gssi2021"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
